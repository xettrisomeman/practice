{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = data.Field(tokenize='spacy' , include_lengths=True)\n",
    "Label = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load imdb dataset\n",
    "\n",
    "train_data , test_data  = datasets.IMDB.splits(Text , Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #70/30 split\n",
    "\n",
    "train_data , valid_data = train_data.split(random_state= random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'how', 'many', 'actors', 'can', 'he', 'get', 'to', 'stand', 'in', 'for', 'his', 'own', 'neurotic', ',', 'compulsive', 'uber', '-', 'New', 'Yorker', 'persona', '?', 'In', 'this', 'film', 'Woody', 'is', 'played', 'by', 'Will', 'Ferrell', 'in', 'what', 'is', 'mercifully', 'less', 'a', 'direct', 'impersonation', 'than', 'the', 'one', 'Kenneth', 'Branagh', 'did', 'in', '\"', 'Celebrity', '.', '\"', 'It', \"'s\", 'an', 'annoyingly', 'repetitive', 'story', 'now', ':', 'nebbishy', ',', 'neurotic', 'man', 'with', 'a', 'wife', 'or', 'girlfriend', 'falls', 'madly', 'in', 'love', 'with', 'a', 'shiksa', 'queen', 'upon', 'which', 'he', 'projects', 'all', 'manner', 'of', 'perfection', '.', 'Everyone', 'lives', 'in', 'perfect', 'gigantic', 'apartments', 'in', 'great', 'Manhattan', 'neighborhoods', ',', 'everyone', 'constantly', 'patronizes', 'expensive', ',', 'exclusive', 'restaurants', 'during', 'which', 'all', 'the', 'characters', 'relate', 'fascinating', 'anecdotes', 'and', 'discuss', 'arcane', 'philosophy', ',', 'there', 'is', 'always', 'a', 'trip', 'to', 'the', 'Hamptons', 'during', 'which', 'the', 'nebbishy', 'main', 'character', 'spazzes', 'out', 'about', 'sand', 'and', 'physical', 'exertion', 'and', 'possible', 'exposure', 'to', 'diseases', ',', 'and', 'then', 'of', 'course', ',', 'said', 'main', 'character', 'feels', 'guilty', 'about', 'his', 'lust', 'for', 'the', 'shiksa', 'queen', 'but', 'pursues', 'her', 'anyway', ',', 'sometimes', 'succeeding', ',', 'sometimes', 'failing', ',', 'etc.<br', '/><br', '/>This', 'a', 'tired', 'formula', ',', 'and', 'proof', 'that', 'Allen', 'is', \"n't\", 'really', 'a', 'great', 'film', 'artist', 'at', 'all', '.', 'He', 'just', 'seems', 'like', 'a', 'dirty', 'old', 'man', 'with', 'the', 'libido', 'and', 'emotions', 'of', 'a', '20-year', '-', 'old', 'who', 'is', 'intent', 'upon', 'telling', 'the', 'same', 'boring', 'old', 'stories', 'again', 'and', 'again', '.']\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "for data in train_data:\n",
    "    print(data.text)\n",
    "    print(data.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use glove vectors\n",
    "\n",
    "MAX_VOCAB_SIZE=25000\n",
    "\n",
    "Text.build_vocab(\n",
    "train_data , \n",
    "    max_size=MAX_VOCAB_SIZE,\n",
    "    vectors ='glove.6B.100d',\n",
    "    unk_init=torch.Tensor.normal_\n",
    ")\n",
    "\n",
    "Label.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "Batch_size=64\n",
    "\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = Batch_size,\n",
    "    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self , vocab_size , embedding_dim , \n",
    "                hidden_dim , output_dim , n_layers ,bidirectional , dropout , pad_idx ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding =nn.Embedding(vocab_size , embedding_dim ,padding_idx=pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim , hidden_dim , num_layers=n_layers , bidirectional=bidirectional , dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim*2 , output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self,text,text_lengths):\n",
    "        \n",
    "        #text = [sent len , batch_size]\n",
    "        \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        #embedded = [sent len , batchsize, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded , text_lengths)\n",
    "        \n",
    "        packed_output , (hidden ,cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        \n",
    "        output , output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        \n",
    "        #output = [sent len , batch size , hid dim * num directions]\n",
    "        #output over padding tokens are zero tokens\n",
    "        \n",
    "        #hidden = [num layrs * num directions , batch size , hid dim]\n",
    "        \n",
    "        #cell = [num layers * num directions , batch size , hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:])\n",
    "        # hidden layers\n",
    "        #apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:] , hidden[-1,:,:]) , dim=1))\n",
    "        \n",
    "        #hidden = [batch size , hid dim * num directions]\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(Text.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM=1\n",
    "N_LAYERS=2\n",
    "BIDIRECTIONAL=True\n",
    "DROPOUT=0.5\n",
    "PAD_IDX = Text.vocab.stoi[Text.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM , \n",
    "           EMBEDDING_DIM , \n",
    "           HIDDEN_DIM,\n",
    "           OUTPUT_DIM,\n",
    "           N_LAYERS,\n",
    "           BIDIRECTIONAL,\n",
    "           DROPOUT,\n",
    "           PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
       "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The model has  4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "#print number of trainable parameters\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f' The model has {count_parameters(model): ,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The final addition is copying the pre-trained word embeddings we loaded earlier into the embedding layer of our model.\n",
    "\n",
    "We retrieve the embeddings from the field's vocab, and check they're the correct size, [vocab size, embedding dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = Text.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We then replace the initial weights of the embedding layer with the pre-trained embeddings.\n",
    "\n",
    "Note: this should always be done on the weight.data and not the weight!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4611, -0.0639, -1.3667,  ...,  1.6309, -0.0847,  1.0844],\n",
       "        [ 0.1954, -1.3350,  0.3945,  ..., -0.9228, -1.2620,  1.0861],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.7836,  0.9079,  0.9177,  ...,  0.7037,  1.4912,  0.5677],\n",
       "        [ 0.1329,  0.1716,  0.7947,  ..., -0.4911, -0.1513,  0.3190],\n",
       "        [ 0.2416,  0.0783, -0.8522,  ..., -0.1595,  0.6774,  0.2029]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As our unk and pad token aren't in the pre-trained vocabulary they have been initialized using unk_init (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. It is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment.\n",
    "\n",
    "We do this by manually setting their row in the embedding weights matrix to zeros. We get their row by finding the index of the tokens, which we have already done for the padding index.\n",
    "\n",
    "Note: like initializing the embeddings, this should be done on the weight.data and not the weight!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = Text.vocab.stoi[Text.unk_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.7836,  0.9079,  0.9177,  ...,  0.7037,  1.4912,  0.5677],\n",
      "        [ 0.1329,  0.1716,  0.7947,  ..., -0.4911, -0.1513,  0.3190],\n",
      "        [ 0.2416,  0.0783, -0.8522,  ..., -0.1595,  0.6774,  0.2029]])\n"
     ]
    }
   ],
   "source": [
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the padding_idx of the embedding layer it will remain zeros throughout training, however the <unk> token embedding will be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds , y):\n",
    "    \n",
    "    #round predictions to the closet integer\n",
    "    \n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    \n",
    "    correct = (rounded_preds == y).float()\n",
    "    \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model , iterator , optimizer,criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text , text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text , text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions , batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions , batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator) , epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model , iterator , criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    epoch_acc =0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in tqdm(iterator):\n",
    "            \n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text , text_lengths).squeeze(1) #->[batch size]\n",
    "            \n",
    "            loss = criterion(predictions , batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions , batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss/len(iterator) , epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def epoch_time(start_time , end_time):\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     elapsed_mins = int(elapsed_time/60)\n",
    "#     elapsed_secs = int(elapsed_time - (elapsed_mins*60))\n",
    "#     return elapsed_mins , elapsed_secs\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 2/274 [00:31<51:05, 11.27s/it]"
     ]
    }
   ],
   "source": [
    "N_epochs = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "    train_loss , train_acc = train(model , train_iterator , optimizer,criterion)\n",
    "    valid_loss , valid_acc = evaluate(model , valid_iterator , criterion)\n",
    "    \n",
    "#     end_time = time.time()\n",
    "    \n",
    "#     epoch_mins,  epoch_secs = epoch_time(start_time , end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict() , 'tut2-model.pt')\n",
    "        \n",
    "    \n",
    "#     print(f'epoch : {epoch+1:02} | epoch time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain loss: {train_loss:.3f} | Train acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val loss: {valid_loss:.3f} | val acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
